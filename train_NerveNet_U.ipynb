{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver_NerveNet import Solver\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from classifiers.NerveNet_U import NerveNET\n",
    "import transform_utils_NerveNet as tu\n",
    "\n",
    "from data_utils_NerveNet import SegmentationData\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_train': 3383, 'num_val': 1127, 'num_test': 1127} 5635\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "img_files = '../data/image_files.txt'\n",
    "num_lines = sum(1 for line in open(img_files, 'r'))\n",
    "nums = {'num_train' : int(0.6*num_lines)+2,\n",
    "        'num_val'   : int(0.2*num_lines),\n",
    "        'num_test' : int(0.2*num_lines)}\n",
    "\n",
    "print(nums, num_lines)\n",
    "print(np.sum(list(nums.values())) == num_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = (1, 128, 128)\n",
    "W, H = input_dim[1::]\n",
    "W4, H4 = int(W / 2**3), int(H / 2**3)\n",
    "W5, H5 = int(W / 2**4), int(H / 2**4)\n",
    "resize_to_layer4 = (W4, H4)\n",
    "resize_to_layer5 = (W5, H5)\n",
    "\n",
    "transform1 = tu.Resize(input_dim[1::])\n",
    "transform2 = tu.Resize(resize_to_layer4)\n",
    "transform3 = tu.Resize(resize_to_layer5)\n",
    "random_transforms = tu.Compose([\n",
    "                        tu.RandomHorizontalFlip(),\n",
    "                        tu.RandomVerticalFlip(),\n",
    "                        tu.ToTensor()])\n",
    "\n",
    "data_transform = [transform1, transform2, transform3, random_transforms]\n",
    "\n",
    "\n",
    "train_data = SegmentationData(img_files, transform = data_transform, mode = 'train', **nums)\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)\n",
    "\n",
    "val_data = SegmentationData(img_files,  transform = data_transform, mode = 'val', **nums)\n",
    "val_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NerveNET(input_dim, num_classes = 2, weight_scale = 0.01, dropout = 0.15)\n",
    "#model = torch.load(\"models/NerveNet_v1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yy/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([4, 1, 128, 128])) that is different to the input size (torch.Size([4, 128, 128])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/yy/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([4, 1, 8, 8])) that is different to the input size (torch.Size([4, 8, 8])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/yy/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([4, 1, 16, 16])) that is different to the input size (torch.Size([4, 16, 16])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 10/4230] TRAIN loss: 0.804\n",
      "[Iteration 20/4230] TRAIN loss: 0.791\n",
      "[Iteration 30/4230] TRAIN loss: 0.767\n",
      "[Iteration 40/4230] TRAIN loss: 0.723\n",
      "[Iteration 50/4230] TRAIN loss: 0.658\n",
      "[Iteration 60/4230] TRAIN loss: 0.578\n",
      "[Iteration 70/4230] TRAIN loss: 0.491\n",
      "[Iteration 80/4230] TRAIN loss: 0.409\n",
      "[Iteration 90/4230] TRAIN loss: 0.337\n",
      "[Iteration 100/4230] TRAIN loss: 0.287\n",
      "[Iteration 110/4230] TRAIN loss: 0.250\n",
      "[Iteration 120/4230] TRAIN loss: 0.222\n",
      "[Iteration 130/4230] TRAIN loss: 0.197\n",
      "[Iteration 140/4230] TRAIN loss: 0.170\n",
      "[Iteration 150/4230] TRAIN loss: 0.181\n",
      "[Iteration 160/4230] TRAIN loss: 0.162\n",
      "[Iteration 170/4230] TRAIN loss: 0.161\n",
      "[Iteration 180/4230] TRAIN loss: 0.162\n",
      "[Iteration 190/4230] TRAIN loss: 0.149\n",
      "[Iteration 200/4230] TRAIN loss: 0.145\n",
      "[Iteration 210/4230] TRAIN loss: 0.132\n",
      "[Iteration 220/4230] TRAIN loss: 0.157\n",
      "[Iteration 230/4230] TRAIN loss: 0.150\n",
      "[Iteration 240/4230] TRAIN loss: 0.142\n",
      "[Iteration 250/4230] TRAIN loss: 0.132\n",
      "[Iteration 260/4230] TRAIN loss: 0.150\n",
      "[Iteration 270/4230] TRAIN loss: 0.151\n",
      "[Iteration 280/4230] TRAIN loss: 0.139\n",
      "[Iteration 290/4230] TRAIN loss: 0.125\n",
      "[Iteration 300/4230] TRAIN loss: 0.118\n",
      "[Iteration 310/4230] TRAIN loss: 0.142\n",
      "[Iteration 320/4230] TRAIN loss: 0.135\n",
      "[Iteration 330/4230] TRAIN loss: 0.128\n",
      "[Iteration 340/4230] TRAIN loss: 0.116\n",
      "[Iteration 350/4230] TRAIN loss: 0.140\n",
      "[Iteration 360/4230] TRAIN loss: 0.143\n",
      "[Iteration 370/4230] TRAIN loss: 0.120\n",
      "[Iteration 380/4230] TRAIN loss: 0.127\n",
      "[Iteration 390/4230] TRAIN loss: 0.131\n",
      "[Iteration 400/4230] TRAIN loss: 0.149\n",
      "[Iteration 410/4230] TRAIN loss: 0.132\n",
      "[Iteration 420/4230] TRAIN loss: 0.138\n",
      "[Iteration 430/4230] TRAIN loss: 0.128\n",
      "[Iteration 440/4230] TRAIN loss: 0.140\n",
      "[Iteration 450/4230] TRAIN loss: 0.131\n",
      "[Iteration 460/4230] TRAIN loss: 0.115\n",
      "[Iteration 470/4230] TRAIN loss: 0.131\n",
      "[Iteration 480/4230] TRAIN loss: 0.117\n",
      "[Iteration 490/4230] TRAIN loss: 0.124\n",
      "[Iteration 500/4230] TRAIN loss: 0.139\n",
      "[Iteration 510/4230] TRAIN loss: 0.132\n",
      "[Iteration 520/4230] TRAIN loss: 0.117\n",
      "[Iteration 530/4230] TRAIN loss: 0.139\n",
      "[Iteration 540/4230] TRAIN loss: 0.131\n",
      "[Iteration 550/4230] TRAIN loss: 0.116\n",
      "[Iteration 560/4230] TRAIN loss: 0.117\n",
      "[Iteration 570/4230] TRAIN loss: 0.126\n",
      "[Iteration 580/4230] TRAIN loss: 0.130\n",
      "[Iteration 590/4230] TRAIN loss: 0.124\n",
      "[Iteration 600/4230] TRAIN loss: 0.110\n",
      "[Iteration 610/4230] TRAIN loss: 0.114\n",
      "[Iteration 620/4230] TRAIN loss: 0.121\n",
      "[Iteration 630/4230] TRAIN loss: 0.120\n",
      "[Iteration 640/4230] TRAIN loss: 0.111\n",
      "[Iteration 650/4230] TRAIN loss: 0.120\n",
      "[Iteration 660/4230] TRAIN loss: 0.125\n",
      "[Iteration 670/4230] TRAIN loss: 0.133\n",
      "[Iteration 680/4230] TRAIN loss: 0.120\n",
      "[Iteration 690/4230] TRAIN loss: 0.131\n",
      "[Iteration 700/4230] TRAIN loss: 0.107\n",
      "[Iteration 710/4230] TRAIN loss: 0.103\n",
      "[Iteration 720/4230] TRAIN loss: 0.141\n",
      "[Iteration 730/4230] TRAIN loss: 0.129\n",
      "[Iteration 740/4230] TRAIN loss: 0.119\n",
      "[Iteration 750/4230] TRAIN loss: 0.117\n",
      "[Iteration 760/4230] TRAIN loss: 0.105\n",
      "[Iteration 770/4230] TRAIN loss: 0.123\n",
      "[Iteration 780/4230] TRAIN loss: 0.112\n",
      "[Iteration 790/4230] TRAIN loss: 0.105\n",
      "[Iteration 800/4230] TRAIN loss: 0.126\n",
      "[Iteration 810/4230] TRAIN loss: 0.112\n",
      "[Iteration 820/4230] TRAIN loss: 0.118\n",
      "[Iteration 830/4230] TRAIN loss: 0.111\n",
      "[Iteration 840/4230] TRAIN loss: 0.119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yy/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([3, 1, 128, 128])) that is different to the input size (torch.Size([3, 128, 128])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/yy/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([3, 1, 8, 8])) that is different to the input size (torch.Size([3, 8, 8])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/yy/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1168: UserWarning: Using a target size (torch.Size([3, 1, 16, 16])) that is different to the input size (torch.Size([3, 16, 16])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] TRAIN acc/loss: 0.288/0.119\n",
      "[Epoch 1/5] VAL   acc/loss: 0.086/0.139\n",
      "[Iteration 856/4230] TRAIN loss: 0.111\n",
      "[Iteration 866/4230] TRAIN loss: 0.117\n",
      "[Iteration 876/4230] TRAIN loss: 0.110\n",
      "[Iteration 886/4230] TRAIN loss: 0.122\n",
      "[Iteration 896/4230] TRAIN loss: 0.132\n",
      "[Iteration 906/4230] TRAIN loss: 0.134\n",
      "[Iteration 916/4230] TRAIN loss: 0.117\n",
      "[Iteration 926/4230] TRAIN loss: 0.103\n",
      "[Iteration 936/4230] TRAIN loss: 0.123\n",
      "[Iteration 946/4230] TRAIN loss: 0.109\n",
      "[Iteration 956/4230] TRAIN loss: 0.133\n",
      "[Iteration 966/4230] TRAIN loss: 0.128\n",
      "[Iteration 976/4230] TRAIN loss: 0.118\n",
      "[Iteration 986/4230] TRAIN loss: 0.099\n",
      "[Iteration 996/4230] TRAIN loss: 0.110\n",
      "[Iteration 1006/4230] TRAIN loss: 0.116\n",
      "[Iteration 1016/4230] TRAIN loss: 0.120\n",
      "[Iteration 1026/4230] TRAIN loss: 0.110\n",
      "[Iteration 1036/4230] TRAIN loss: 0.113\n",
      "[Iteration 1046/4230] TRAIN loss: 0.127\n",
      "[Iteration 1056/4230] TRAIN loss: 0.113\n",
      "[Iteration 1066/4230] TRAIN loss: 0.097\n",
      "[Iteration 1076/4230] TRAIN loss: 0.100\n",
      "[Iteration 1086/4230] TRAIN loss: 0.104\n",
      "[Iteration 1096/4230] TRAIN loss: 0.117\n",
      "[Iteration 1106/4230] TRAIN loss: 0.118\n",
      "[Iteration 1116/4230] TRAIN loss: 0.113\n",
      "[Iteration 1126/4230] TRAIN loss: 0.101\n",
      "[Iteration 1136/4230] TRAIN loss: 0.125\n",
      "[Iteration 1146/4230] TRAIN loss: 0.095\n",
      "[Iteration 1156/4230] TRAIN loss: 0.102\n",
      "[Iteration 1166/4230] TRAIN loss: 0.101\n",
      "[Iteration 1176/4230] TRAIN loss: 0.108\n",
      "[Iteration 1186/4230] TRAIN loss: 0.145\n",
      "[Iteration 1196/4230] TRAIN loss: 0.101\n",
      "[Iteration 1206/4230] TRAIN loss: 0.106\n",
      "[Iteration 1216/4230] TRAIN loss: 0.114\n",
      "[Iteration 1226/4230] TRAIN loss: 0.099\n",
      "[Iteration 1236/4230] TRAIN loss: 0.107\n",
      "[Iteration 1246/4230] TRAIN loss: 0.105\n",
      "[Iteration 1256/4230] TRAIN loss: 0.085\n",
      "[Iteration 1266/4230] TRAIN loss: 0.110\n",
      "[Iteration 1276/4230] TRAIN loss: 0.121\n",
      "[Iteration 1286/4230] TRAIN loss: 0.136\n",
      "[Iteration 1296/4230] TRAIN loss: 0.120\n",
      "[Iteration 1306/4230] TRAIN loss: 0.104\n",
      "[Iteration 1316/4230] TRAIN loss: 0.128\n",
      "[Iteration 1326/4230] TRAIN loss: 0.118\n",
      "[Iteration 1336/4230] TRAIN loss: 0.146\n",
      "[Iteration 1346/4230] TRAIN loss: 0.133\n",
      "[Iteration 1356/4230] TRAIN loss: 0.101\n",
      "[Iteration 1366/4230] TRAIN loss: 0.121\n",
      "[Iteration 1376/4230] TRAIN loss: 0.122\n",
      "[Iteration 1386/4230] TRAIN loss: 0.116\n",
      "[Iteration 1396/4230] TRAIN loss: 0.093\n",
      "[Iteration 1406/4230] TRAIN loss: 0.117\n",
      "[Iteration 1416/4230] TRAIN loss: 0.092\n",
      "[Iteration 1426/4230] TRAIN loss: 0.118\n",
      "[Iteration 1436/4230] TRAIN loss: 0.104\n",
      "[Iteration 1446/4230] TRAIN loss: 0.107\n",
      "[Iteration 1456/4230] TRAIN loss: 0.133\n",
      "[Iteration 1466/4230] TRAIN loss: 0.087\n",
      "[Iteration 1476/4230] TRAIN loss: 0.131\n",
      "[Iteration 1486/4230] TRAIN loss: 0.127\n",
      "[Iteration 1496/4230] TRAIN loss: 0.107\n",
      "[Iteration 1506/4230] TRAIN loss: 0.093\n",
      "[Iteration 1516/4230] TRAIN loss: 0.118\n",
      "[Iteration 1526/4230] TRAIN loss: 0.104\n",
      "[Iteration 1536/4230] TRAIN loss: 0.089\n",
      "[Iteration 1546/4230] TRAIN loss: 0.109\n",
      "[Iteration 1556/4230] TRAIN loss: 0.115\n",
      "[Iteration 1566/4230] TRAIN loss: 0.113\n",
      "[Iteration 1576/4230] TRAIN loss: 0.088\n",
      "[Iteration 1586/4230] TRAIN loss: 0.125\n",
      "[Iteration 1596/4230] TRAIN loss: 0.132\n",
      "[Iteration 1606/4230] TRAIN loss: 0.096\n",
      "[Iteration 1616/4230] TRAIN loss: 0.104\n",
      "[Iteration 1626/4230] TRAIN loss: 0.078\n",
      "[Iteration 1636/4230] TRAIN loss: 0.098\n",
      "[Iteration 1646/4230] TRAIN loss: 0.114\n",
      "[Iteration 1656/4230] TRAIN loss: 0.110\n",
      "[Iteration 1666/4230] TRAIN loss: 0.116\n",
      "[Iteration 1676/4230] TRAIN loss: 0.107\n",
      "[Iteration 1686/4230] TRAIN loss: 0.101\n",
      "[Epoch 2/5] TRAIN acc/loss: 0.200/0.101\n",
      "[Epoch 2/5] VAL   acc/loss: 0.199/0.119\n",
      "[Iteration 1702/4230] TRAIN loss: 0.118\n",
      "[Iteration 1712/4230] TRAIN loss: 0.105\n",
      "[Iteration 1722/4230] TRAIN loss: 0.087\n",
      "[Iteration 1732/4230] TRAIN loss: 0.093\n",
      "[Iteration 1742/4230] TRAIN loss: 0.099\n",
      "[Iteration 1752/4230] TRAIN loss: 0.094\n",
      "[Iteration 1762/4230] TRAIN loss: 0.102\n",
      "[Iteration 1772/4230] TRAIN loss: 0.104\n",
      "[Iteration 1782/4230] TRAIN loss: 0.097\n",
      "[Iteration 1792/4230] TRAIN loss: 0.091\n",
      "[Iteration 1802/4230] TRAIN loss: 0.121\n",
      "[Iteration 1812/4230] TRAIN loss: 0.120\n",
      "[Iteration 1822/4230] TRAIN loss: 0.088\n",
      "[Iteration 1832/4230] TRAIN loss: 0.128\n",
      "[Iteration 1842/4230] TRAIN loss: 0.096\n",
      "[Iteration 1852/4230] TRAIN loss: 0.126\n",
      "[Iteration 1862/4230] TRAIN loss: 0.101\n",
      "[Iteration 1872/4230] TRAIN loss: 0.119\n",
      "[Iteration 1882/4230] TRAIN loss: 0.106\n",
      "[Iteration 1892/4230] TRAIN loss: 0.097\n",
      "[Iteration 1902/4230] TRAIN loss: 0.085\n",
      "[Iteration 1912/4230] TRAIN loss: 0.118\n",
      "[Iteration 1922/4230] TRAIN loss: 0.088\n",
      "[Iteration 1932/4230] TRAIN loss: 0.119\n",
      "[Iteration 1942/4230] TRAIN loss: 0.091\n",
      "[Iteration 1952/4230] TRAIN loss: 0.099\n",
      "[Iteration 1962/4230] TRAIN loss: 0.132\n",
      "[Iteration 1972/4230] TRAIN loss: 0.110\n",
      "[Iteration 1982/4230] TRAIN loss: 0.144\n",
      "[Iteration 1992/4230] TRAIN loss: 0.090\n",
      "[Iteration 2002/4230] TRAIN loss: 0.102\n",
      "[Iteration 2012/4230] TRAIN loss: 0.091\n",
      "[Iteration 2022/4230] TRAIN loss: 0.136\n",
      "[Iteration 2032/4230] TRAIN loss: 0.122\n",
      "[Iteration 2042/4230] TRAIN loss: 0.105\n",
      "[Iteration 2052/4230] TRAIN loss: 0.115\n",
      "[Iteration 2062/4230] TRAIN loss: 0.095\n",
      "[Iteration 2072/4230] TRAIN loss: 0.119\n",
      "[Iteration 2082/4230] TRAIN loss: 0.109\n",
      "[Iteration 2092/4230] TRAIN loss: 0.085\n",
      "[Iteration 2102/4230] TRAIN loss: 0.108\n",
      "[Iteration 2112/4230] TRAIN loss: 0.094\n",
      "[Iteration 2122/4230] TRAIN loss: 0.125\n",
      "[Iteration 2132/4230] TRAIN loss: 0.094\n",
      "[Iteration 2142/4230] TRAIN loss: 0.107\n",
      "[Iteration 2152/4230] TRAIN loss: 0.115\n",
      "[Iteration 2162/4230] TRAIN loss: 0.108\n",
      "[Iteration 2172/4230] TRAIN loss: 0.084\n",
      "[Iteration 2182/4230] TRAIN loss: 0.088\n",
      "[Iteration 2192/4230] TRAIN loss: 0.110\n",
      "[Iteration 2202/4230] TRAIN loss: 0.090\n",
      "[Iteration 2212/4230] TRAIN loss: 0.113\n",
      "[Iteration 2222/4230] TRAIN loss: 0.120\n",
      "[Iteration 2232/4230] TRAIN loss: 0.100\n",
      "[Iteration 2242/4230] TRAIN loss: 0.086\n",
      "[Iteration 2252/4230] TRAIN loss: 0.102\n",
      "[Iteration 2262/4230] TRAIN loss: 0.112\n",
      "[Iteration 2272/4230] TRAIN loss: 0.115\n",
      "[Iteration 2282/4230] TRAIN loss: 0.081\n",
      "[Iteration 2292/4230] TRAIN loss: 0.111\n",
      "[Iteration 2302/4230] TRAIN loss: 0.154\n",
      "[Iteration 2312/4230] TRAIN loss: 0.128\n",
      "[Iteration 2322/4230] TRAIN loss: 0.097\n",
      "[Iteration 2332/4230] TRAIN loss: 0.116\n",
      "[Iteration 2342/4230] TRAIN loss: 0.106\n",
      "[Iteration 2352/4230] TRAIN loss: 0.094\n",
      "[Iteration 2362/4230] TRAIN loss: 0.100\n",
      "[Iteration 2372/4230] TRAIN loss: 0.108\n",
      "[Iteration 2382/4230] TRAIN loss: 0.112\n",
      "[Iteration 2392/4230] TRAIN loss: 0.103\n",
      "[Iteration 2402/4230] TRAIN loss: 0.131\n",
      "[Iteration 2412/4230] TRAIN loss: 0.104\n",
      "[Iteration 2422/4230] TRAIN loss: 0.115\n",
      "[Iteration 2432/4230] TRAIN loss: 0.102\n",
      "[Iteration 2442/4230] TRAIN loss: 0.122\n",
      "[Iteration 2452/4230] TRAIN loss: 0.115\n",
      "[Iteration 2462/4230] TRAIN loss: 0.113\n",
      "[Iteration 2472/4230] TRAIN loss: 0.108\n",
      "[Iteration 2482/4230] TRAIN loss: 0.107\n",
      "[Iteration 2492/4230] TRAIN loss: 0.097\n",
      "[Iteration 2502/4230] TRAIN loss: 0.118\n",
      "[Iteration 2512/4230] TRAIN loss: 0.123\n",
      "[Iteration 2522/4230] TRAIN loss: 0.112\n",
      "[Iteration 2532/4230] TRAIN loss: 0.090\n",
      "[Epoch 3/5] TRAIN acc/loss: 0.217/0.090\n",
      "[Epoch 3/5] VAL   acc/loss: 0.093/0.145\n",
      "[Iteration 2548/4230] TRAIN loss: 0.076\n",
      "[Iteration 2558/4230] TRAIN loss: 0.086\n",
      "[Iteration 2568/4230] TRAIN loss: 0.106\n",
      "[Iteration 2578/4230] TRAIN loss: 0.113\n",
      "[Iteration 2588/4230] TRAIN loss: 0.092\n",
      "[Iteration 2598/4230] TRAIN loss: 0.120\n",
      "[Iteration 2608/4230] TRAIN loss: 0.119\n",
      "[Iteration 2618/4230] TRAIN loss: 0.125\n",
      "[Iteration 2628/4230] TRAIN loss: 0.114\n",
      "[Iteration 2638/4230] TRAIN loss: 0.095\n",
      "[Iteration 2648/4230] TRAIN loss: 0.104\n",
      "[Iteration 2658/4230] TRAIN loss: 0.103\n",
      "[Iteration 2668/4230] TRAIN loss: 0.102\n",
      "[Iteration 2678/4230] TRAIN loss: 0.092\n",
      "[Iteration 2688/4230] TRAIN loss: 0.106\n",
      "[Iteration 2698/4230] TRAIN loss: 0.125\n",
      "[Iteration 2708/4230] TRAIN loss: 0.097\n",
      "[Iteration 2718/4230] TRAIN loss: 0.116\n",
      "[Iteration 2728/4230] TRAIN loss: 0.094\n",
      "[Iteration 2738/4230] TRAIN loss: 0.128\n",
      "[Iteration 2748/4230] TRAIN loss: 0.115\n",
      "[Iteration 2758/4230] TRAIN loss: 0.095\n",
      "[Iteration 2768/4230] TRAIN loss: 0.113\n",
      "[Iteration 2778/4230] TRAIN loss: 0.117\n",
      "[Iteration 2788/4230] TRAIN loss: 0.107\n",
      "[Iteration 2798/4230] TRAIN loss: 0.092\n",
      "[Iteration 2808/4230] TRAIN loss: 0.129\n",
      "[Iteration 2818/4230] TRAIN loss: 0.119\n",
      "[Iteration 2828/4230] TRAIN loss: 0.100\n",
      "[Iteration 2838/4230] TRAIN loss: 0.112\n",
      "[Iteration 2848/4230] TRAIN loss: 0.108\n",
      "[Iteration 2858/4230] TRAIN loss: 0.087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 2868/4230] TRAIN loss: 0.104\n",
      "[Iteration 2878/4230] TRAIN loss: 0.122\n",
      "[Iteration 2888/4230] TRAIN loss: 0.101\n",
      "[Iteration 2898/4230] TRAIN loss: 0.101\n",
      "[Iteration 2908/4230] TRAIN loss: 0.094\n",
      "[Iteration 2918/4230] TRAIN loss: 0.091\n",
      "[Iteration 2928/4230] TRAIN loss: 0.107\n",
      "[Iteration 2938/4230] TRAIN loss: 0.113\n",
      "[Iteration 2948/4230] TRAIN loss: 0.084\n",
      "[Iteration 2958/4230] TRAIN loss: 0.106\n",
      "[Iteration 2968/4230] TRAIN loss: 0.109\n",
      "[Iteration 2978/4230] TRAIN loss: 0.097\n",
      "[Iteration 2988/4230] TRAIN loss: 0.124\n",
      "[Iteration 2998/4230] TRAIN loss: 0.101\n",
      "[Iteration 3008/4230] TRAIN loss: 0.108\n",
      "[Iteration 3018/4230] TRAIN loss: 0.108\n",
      "[Iteration 3028/4230] TRAIN loss: 0.100\n",
      "[Iteration 3038/4230] TRAIN loss: 0.123\n",
      "[Iteration 3048/4230] TRAIN loss: 0.103\n",
      "[Iteration 3058/4230] TRAIN loss: 0.116\n",
      "[Iteration 3068/4230] TRAIN loss: 0.093\n",
      "[Iteration 3078/4230] TRAIN loss: 0.124\n",
      "[Iteration 3088/4230] TRAIN loss: 0.096\n",
      "[Iteration 3098/4230] TRAIN loss: 0.099\n",
      "[Iteration 3108/4230] TRAIN loss: 0.092\n",
      "[Iteration 3118/4230] TRAIN loss: 0.092\n",
      "[Iteration 3128/4230] TRAIN loss: 0.093\n",
      "[Iteration 3138/4230] TRAIN loss: 0.097\n",
      "[Iteration 3148/4230] TRAIN loss: 0.113\n",
      "[Iteration 3158/4230] TRAIN loss: 0.099\n",
      "[Iteration 3168/4230] TRAIN loss: 0.114\n",
      "[Iteration 3178/4230] TRAIN loss: 0.095\n",
      "[Iteration 3188/4230] TRAIN loss: 0.103\n",
      "[Iteration 3198/4230] TRAIN loss: 0.110\n",
      "[Iteration 3208/4230] TRAIN loss: 0.101\n",
      "[Iteration 3218/4230] TRAIN loss: 0.098\n",
      "[Iteration 3228/4230] TRAIN loss: 0.097\n",
      "[Iteration 3238/4230] TRAIN loss: 0.095\n",
      "[Iteration 3248/4230] TRAIN loss: 0.109\n",
      "[Iteration 3258/4230] TRAIN loss: 0.096\n",
      "[Iteration 3268/4230] TRAIN loss: 0.081\n",
      "[Iteration 3278/4230] TRAIN loss: 0.102\n",
      "[Iteration 3288/4230] TRAIN loss: 0.088\n",
      "[Iteration 3298/4230] TRAIN loss: 0.113\n",
      "[Iteration 3308/4230] TRAIN loss: 0.098\n",
      "[Iteration 3318/4230] TRAIN loss: 0.105\n",
      "[Iteration 3328/4230] TRAIN loss: 0.084\n",
      "[Iteration 3338/4230] TRAIN loss: 0.093\n",
      "[Iteration 3348/4230] TRAIN loss: 0.080\n",
      "[Iteration 3358/4230] TRAIN loss: 0.104\n",
      "[Iteration 3368/4230] TRAIN loss: 0.121\n",
      "[Iteration 3378/4230] TRAIN loss: 0.107\n",
      "[Epoch 4/5] TRAIN acc/loss: 0.247/0.107\n",
      "[Epoch 4/5] VAL   acc/loss: 0.251/0.109\n",
      "[Iteration 3394/4230] TRAIN loss: 0.100\n",
      "[Iteration 3404/4230] TRAIN loss: 0.107\n",
      "[Iteration 3414/4230] TRAIN loss: 0.117\n",
      "[Iteration 3424/4230] TRAIN loss: 0.102\n",
      "[Iteration 3434/4230] TRAIN loss: 0.103\n",
      "[Iteration 3444/4230] TRAIN loss: 0.091\n",
      "[Iteration 3454/4230] TRAIN loss: 0.083\n",
      "[Iteration 3464/4230] TRAIN loss: 0.109\n",
      "[Iteration 3474/4230] TRAIN loss: 0.103\n",
      "[Iteration 3484/4230] TRAIN loss: 0.089\n",
      "[Iteration 3494/4230] TRAIN loss: 0.098\n",
      "[Iteration 3504/4230] TRAIN loss: 0.112\n",
      "[Iteration 3514/4230] TRAIN loss: 0.108\n",
      "[Iteration 3524/4230] TRAIN loss: 0.129\n",
      "[Iteration 3534/4230] TRAIN loss: 0.125\n",
      "[Iteration 3544/4230] TRAIN loss: 0.089\n",
      "[Iteration 3554/4230] TRAIN loss: 0.099\n",
      "[Iteration 3564/4230] TRAIN loss: 0.127\n",
      "[Iteration 3574/4230] TRAIN loss: 0.085\n",
      "[Iteration 3584/4230] TRAIN loss: 0.095\n",
      "[Iteration 3594/4230] TRAIN loss: 0.098\n",
      "[Iteration 3604/4230] TRAIN loss: 0.140\n",
      "[Iteration 3614/4230] TRAIN loss: 0.086\n",
      "[Iteration 3624/4230] TRAIN loss: 0.105\n",
      "[Iteration 3634/4230] TRAIN loss: 0.116\n",
      "[Iteration 3644/4230] TRAIN loss: 0.104\n",
      "[Iteration 3654/4230] TRAIN loss: 0.109\n",
      "[Iteration 3664/4230] TRAIN loss: 0.090\n",
      "[Iteration 3674/4230] TRAIN loss: 0.108\n",
      "[Iteration 3684/4230] TRAIN loss: 0.106\n",
      "[Iteration 3694/4230] TRAIN loss: 0.087\n",
      "[Iteration 3704/4230] TRAIN loss: 0.089\n",
      "[Iteration 3714/4230] TRAIN loss: 0.128\n",
      "[Iteration 3724/4230] TRAIN loss: 0.116\n",
      "[Iteration 3734/4230] TRAIN loss: 0.092\n",
      "[Iteration 3744/4230] TRAIN loss: 0.095\n",
      "[Iteration 3754/4230] TRAIN loss: 0.097\n",
      "[Iteration 3764/4230] TRAIN loss: 0.127\n",
      "[Iteration 3774/4230] TRAIN loss: 0.110\n",
      "[Iteration 3784/4230] TRAIN loss: 0.103\n",
      "[Iteration 3794/4230] TRAIN loss: 0.115\n",
      "[Iteration 3804/4230] TRAIN loss: 0.103\n",
      "[Iteration 3814/4230] TRAIN loss: 0.106\n",
      "[Iteration 3824/4230] TRAIN loss: 0.108\n",
      "[Iteration 3834/4230] TRAIN loss: 0.104\n",
      "[Iteration 3844/4230] TRAIN loss: 0.109\n",
      "[Iteration 3854/4230] TRAIN loss: 0.096\n",
      "[Iteration 3864/4230] TRAIN loss: 0.100\n",
      "[Iteration 3874/4230] TRAIN loss: 0.106\n",
      "[Iteration 3884/4230] TRAIN loss: 0.130\n",
      "[Iteration 3894/4230] TRAIN loss: 0.109\n",
      "[Iteration 3904/4230] TRAIN loss: 0.092\n",
      "[Iteration 3914/4230] TRAIN loss: 0.105\n",
      "[Iteration 3924/4230] TRAIN loss: 0.098\n",
      "[Iteration 3934/4230] TRAIN loss: 0.121\n",
      "[Iteration 3944/4230] TRAIN loss: 0.123\n",
      "[Iteration 3954/4230] TRAIN loss: 0.124\n",
      "[Iteration 3964/4230] TRAIN loss: 0.100\n",
      "[Iteration 3974/4230] TRAIN loss: 0.102\n",
      "[Iteration 3984/4230] TRAIN loss: 0.094\n",
      "[Iteration 3994/4230] TRAIN loss: 0.102\n",
      "[Iteration 4004/4230] TRAIN loss: 0.098\n",
      "[Iteration 4014/4230] TRAIN loss: 0.084\n",
      "[Iteration 4024/4230] TRAIN loss: 0.104\n",
      "[Iteration 4034/4230] TRAIN loss: 0.083\n",
      "[Iteration 4044/4230] TRAIN loss: 0.102\n",
      "[Iteration 4054/4230] TRAIN loss: 0.076\n",
      "[Iteration 4064/4230] TRAIN loss: 0.123\n",
      "[Iteration 4074/4230] TRAIN loss: 0.088\n",
      "[Iteration 4084/4230] TRAIN loss: 0.094\n",
      "[Iteration 4094/4230] TRAIN loss: 0.111\n",
      "[Iteration 4104/4230] TRAIN loss: 0.109\n",
      "[Iteration 4114/4230] TRAIN loss: 0.084\n",
      "[Iteration 4124/4230] TRAIN loss: 0.084\n",
      "[Iteration 4134/4230] TRAIN loss: 0.090\n",
      "[Iteration 4144/4230] TRAIN loss: 0.092\n",
      "[Iteration 4154/4230] TRAIN loss: 0.104\n",
      "[Iteration 4164/4230] TRAIN loss: 0.089\n",
      "[Iteration 4174/4230] TRAIN loss: 0.103\n",
      "[Iteration 4184/4230] TRAIN loss: 0.098\n",
      "[Iteration 4194/4230] TRAIN loss: 0.097\n",
      "[Iteration 4204/4230] TRAIN loss: 0.085\n",
      "[Iteration 4214/4230] TRAIN loss: 0.106\n",
      "[Iteration 4224/4230] TRAIN loss: 0.111\n",
      "[Epoch 5/5] TRAIN acc/loss: 0.632/0.111\n",
      "[Epoch 5/5] VAL   acc/loss: 0.250/0.114\n",
      "FINISH.\n"
     ]
    }
   ],
   "source": [
    "#for param in list(model.parameters()):\n",
    "#    param.requires_grad = False\n",
    "    \n",
    "#for param in list(model.parameters())[-2::]:\n",
    "#    param.requires_grad = True\n",
    "\n",
    "solver = Solver(optim_args={\"lr\": 0.001, #0.0025, #1.e-3, #1.e-2\n",
    "                            \"betas\": (0.9, 0.999),\n",
    "                            \"eps\": 1e-8,\n",
    "                            \"weight_decay\": 0.},\n",
    "                loss_func = torch.nn.BCELoss(), loss_weights = [1.0, 0.1, 0.05, 0.01])\n",
    "\n",
    "#print(model)\n",
    "outputs = solver.train(model, train_loader, val_loader, log_nth=10, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model... models/NerveNet_U_v1.model\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/NerveNet_U_v1.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b571a13a2e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/NerveNet_U_v1.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DL4CV_project/nervenet/classifiers/NerveNet_U.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[1;32m    184\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving model... %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mto\u001b[0m \u001b[0moverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/NerveNet_U_v1.model'"
     ]
    }
   ],
   "source": [
    "model.save(\"models/NerveNet_U_v1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/clean_no_zero_full_dataset_1300.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9189f4dd0f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSegmentationData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_files\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m test_loader = torch.utils.data.DataLoader(test_data,\n\u001b[1;32m      7\u001b[0m                                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL4CV_project/nervenet/data_utils_NerveNet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_paths_file, transform, mode, num_train, num_val, num_test)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_paths_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m#with open(image_paths_file) as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#    self.image_names = f.read().splitlines()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL4CV_project/nervenet/data_utils_NerveNet.py\u001b[0m in \u001b[0;36mget_image_names\u001b[0;34m(self, mode, image_paths_file)\u001b[0m\n\u001b[1;32m    138\u001b[0m         '''\n\u001b[1;32m    139\u001b[0m         \u001b[0mrandomstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mall_image_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mrandomstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_image_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/clean_no_zero_full_dataset_1300.txt'"
     ]
    }
   ],
   "source": [
    "test_data_transform = transforms.Resize(input_dim[1::])\n",
    "img_files = '../data/image_files.txt'\n",
    "\n",
    "\n",
    "test_data = SegmentationData(img_files,  transform = data_transform, mode = 'test', **nums)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=12,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img, targets = test_data[9]\n",
    "\n",
    "target = targets['in']\n",
    "inputs = img.unsqueeze(0)\n",
    "inputs = Variable(inputs)\n",
    "\n",
    "outputs = model.forward(inputs)\n",
    "pred = outputs[0].squeeze().data.cpu().numpy()\n",
    "\n",
    "img=np.squeeze(img)\n",
    "target = target.squeeze().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(img)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(target)\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(np.where(pred < 0.1, 0, 1))\n",
    "ax[2].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(img)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(target)\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(np.where(pred < 0.5, 0, 1))\n",
    "ax[2].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(pred < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_example_imgs = 4\n",
    "plt.figure(figsize=(15, 5 * num_example_imgs))\n",
    "for i, (img, targets) in enumerate(test_data[1:4]):\n",
    "    \n",
    "    target = targets['in']\n",
    "    inputs = img.unsqueeze(0)\n",
    "    inputs = Variable(inputs)\n",
    "\n",
    "    outputs = model.forward(inputs)\n",
    "    pred = outputs[0].squeeze().data.cpu().numpy()\n",
    "\n",
    "    img=np.squeeze(img)\n",
    "    target = target.squeeze().numpy()\n",
    "        \n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(target)\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "\n",
    "    # pred\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 3)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(pred)\n",
    "    if i == 0:\n",
    "        plt.title(\"Prediction image\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(ground_truth, predicted):\n",
    "    gt = ground_truth\n",
    "    p = predicted\n",
    "    if np.sum(p) + np.sum(gt) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        dice = np.sum(p[gt==1])*2.0 / (np.sum(p) + np.sum(gt))\n",
    "        return dice\n",
    "\n",
    "test_scores = []\n",
    "model.eval()\n",
    "for inputs, targets in test_loader:\n",
    "    inputs, targets = Variable(inputs), Variable(targets['in'])\n",
    "    if model.is_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    \n",
    "    outputs = model.forward(inputs)\n",
    "    preds = outputs[0]\n",
    "    test_scores.append(dice_coefficient(np.squeeze(targets.data.numpy()), np.squeeze(preds.data.numpy())))#np.mean((preds == targets)[targets_mask].data.cpu().numpy()))\n",
    "    \n",
    "model.train()\n",
    "np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
